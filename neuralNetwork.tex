\documentclass[a4paper]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[pagebackref=true,breaklinks=true, letterpaper=true, colorlinks,bookmarks=false]{hyperref} 
\usepackage{amssymb}
\usepackage{outline} \usepackage{pmgraph} \usepackage[normalem]{ulem}
\usepackage{graphicx} \usepackage{verbatim}
% \usepackage{minted} % need `-shell-escape' argument for local compile

\title{
    \vspace*{1in}
    \includegraphics[width=2.75in]{figures/zhenglab-logo} \\
    \vspace*{1.2in}
    \textbf{\huge Weekly Work Report}
    \vspace{0.2in}
}

\author{Yufeng Jiang \\
    \vspace*{0.5in} \\
    \textbf{VISION@OUC} \\
    \vspace*{1in}
}

\date{\today}

\begin{document}

\maketitle
\setcounter{page}{0}
\thispagestyle{empty}
\newpage

\section{The Concept of Neural Network}

The terms deep learning refers to training neural networks. We can explain the neural network by an example of housing price prediction showned at Fig.~\ref{fig4} shown at~\cite{Neural}. Given data about the size of house at the estate market and the purpose is to predict the price via fitting a function. We can put the given data on a Cartesian coordinate system, and fit a straight line to this data using the linear regression. But the prices can never be negative, so at the lower end of the line, we bend the curve at this location to make it end up zero. The new function is Rectified Linear Unit (ReLU) which starts at zero. This can be regard as a single neural network. The size of the house as the input to the neural network which we will call x, and the output is the predicted price which we will call y. They are connected with a node which is a single neuron. The neuron implements the function ReLU.

\begin{figure}[hb]
\begin{center}
\includegraphics[width=8cm]{housePrice.png}
\end{center}
\caption{Housing price prediction.}
\label{fig4}
\end{figure}

At a basis of a single neural network, a larger neural network is stacking many single neural network, just like a single Lego brick. We can also use the last example. In this time, we not only can use the size of house to predict the price, but also can consider the number of bedrooms, zip code and the wealth of the neighborhood. The feature of the neural network is to input, and the output can be got whatever the number of examples in our training set. All things in the middle can be figured out by itself. Nodes in the middle are called hidden units in a neural network, and each of them takes its input of four input features. The advantage of neural networks is that we can get accurate functions that map from x to y if we can give enough training examples.

\section{Supervised Learning for Neural Network}

In supervised learning, we have some input, and we want to map some output by a function. Examples are used at Tab.~\ref{tab} used at~\cite{Neural} to explain supervised learning. In the real estate market, the input can be home features, and the output is the price. In photo tagging, we want to output an index tagging photos from 1 to 1000 with the input is an image. And we can input an audio clip to a neural network, then it can output a text transcript. More complicated problem is autonomous driving, when we input an image and some information from a radar, the result we want to get is position of other cars. There are two categories that can be called regression and classification. The regression problem means to predict results within a continuous output, but the classification problem means to predict results in a discrete output. In other words, the purpose of a regression problem is to map input variables to some continuous function, and in a classification problem, we map input variables to some discrete categories. 

\begin{table}[hb]
\begin{center}
\begin{tabular}{c|c|c}
  \hline
  Input(x) & Output(y) & Application \\ 
  \hline
  Home features & Pricd & Real Estate \\
  Ad, user info & Click on ad? (0/1) & Online Advertising \\
  Image & Object (1,$\dots$,1000) & Photo tagging \\
  Audio & Text transcript & Speech recognition \\
  English & Chinese & Machine translation \\
  Image, Radar info & Position of other cars & Autonomous driving \\
  \hline
\end{tabular}
\end{center}
\caption{Some examples of supervised learning.}
\label{tab}
\end{table}

There are different types of neural network as shown at Fig.~\ref{fig1} explained at~\cite{Neural}. From examples mentioned before, it exactly shows that we use a universally standard neural network architecture. For real estate, it might be a relatively standard neural network. Convolutional neural network (CNN) is mostly used for image application and Recurrent Neural Network (RNN) is used for one-dimensional sequence data like a temporal component at audio. For complex problems like the autonomous driving, we often use a hybrid neural network architecture. 

\begin{figure}[hb]
\begin{center}
\includegraphics[width=12cm]{neuralNetworkExamples.png}
\end{center}
\caption{Neural network examples.}
\label{fig1}
\end{figure}

Machine learning is used at structured data and unstructured data (see Fig.~\ref{fig2})~\cite{Neural}. Structured data means basically databases of data, all inputs have a very well defined meaning. Unstructured data like audio, raw audio or images does not have a clear define that computers are difficult to understand compared to structured data. Instead, people are good at interpreting unstructured data. And now, computers are better at understanding unstructured data due to deep learning. Neural network has transformed supervised learning deeply to create more economic value and the development of the times.

\begin{figure}[hb]
\begin{center}
\includegraphics[width=14cm]{structuredData.png}
\end{center}
\caption{Structured data vs unstructured data.}
\label{fig2}
\end{figure}

\section{Why is Deep Learning Taking Off}

When it comes to the reason why is deep learning taking off, we can give a picture like Fig.~\ref{fig3} shown at~\cite{Neural} to explain it. On the horizontal axis, we plot the amount of labeled data we have for a task, and on the vertical axis, we plot the performance. We put the performance of a traditional learning algorithm as a function of the amount of data. From this figure, the lowest curve where the performance improves as adding data at the first, but it will soon become gentle because this model can not do with a large amount of data. 

\begin{figure}
\begin{center}
\includegraphics[width=14cm]{scale.png}
\end{center}
\caption{Scale drives deep learning progress.}
\label{fig3}
\end{figure}

Over the past 20 years, we have accumulated large amounts of data thanks to the digitization of a society because people spend much time on digital devices to create much data more than traditional learning algorithms. Three higher curves represent the performance of neural network. The more scale the neural network has, the more good performance it will get. Thus, if we want to hit very high level of performance, we need to be able to train a big enough neural network to take advantage of the huge amount of data. In other words, the progress of deep learning cannot leave scale that means the size of the neural network with a lot of hidden units, as well as scale of the data.

In the regime of smaller training sets, algorithms do not have a very clear ranking. So, the performance often depends on the skill at hand engineering features when the training data is not too much. Only in the regime of bigger training sets, we can see the larger neural network domain the other algorithms. 

In the early years, deep learning has been a rising time that the scaled data and scale of computation train very large neural networks to make us a lot of progress. But recently, algorithmic innovations also have a tremendous progress to make neural networks run much faster. We use ReLU function to instead a sigmoid function by changing the activation function to prevent the gradient gradually shrink to zero, in order to make an algorithm called gradient descent work much faster. The final goal we want to get by innovating algorithms is to let code run much faster and train bigger neural networks. 

Mostly, we intuitively have an idea for a neural network architecture, and then writing our idea via code to implement it and run an experiment. According to the results of our neural network does, we have other idea to modify my code and run again. If we can run an experiment in ten minutes or a day rather than a month, we can have more time to try more ideas to find a better neural network to fit our application. So, faster computation has really helped the iterative speed to get a result early, and the iterative speed really affects the productivity of neural networks.


{\small
\bibliographystyle{ieee}
\bibliography{neuralNetwork}
}








\end{document}